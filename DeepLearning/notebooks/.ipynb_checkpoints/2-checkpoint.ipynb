{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723edb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a84ddf",
   "metadata": {},
   "source": [
    "### Likelihood Function\n",
    "\n",
    "We have observations $\\bold{x} = (x_{1}, x_{2}, ..., x_{N})$ suppose that the observations are drawn independently from a gaussian distribution with mean $\\mu$ and variance $\\sigma^{2}$ which are unknown, and we want to determine these parameters from the dataset. The problem of estimating a distribution, given a finite set of observations is **density estimation**. The problem of density estimation is fundamentally ill-posed, because there are infinitely many probability distributions that oculd have given rise to the observed finite data set. Any distribution $p(x)$ that is nonzero at each of the data points $x_{1}, x_{2}, ...$ is a potential candidate. Here we constrain the space of distributions to be Gaussians, which leads to a well-defined solution. \n",
    "\n",
    "Data points that are drawn independently from the same distribution are said to be independent and identically distributed, which is often abbreviated to i.i.d.\n",
    "\n",
    "Because our dataset x is iid, we can write the probability of the dataset given our params:\n",
    "\n",
    "$\\prod\\limits_{n=1}^{N}\\cal{N}(x_{n}|\\mu,\\sigma^{2})$\n",
    "\n",
    "One common approach for determining the paramets in a probability distribution using an observed dataset, known as maximum likelihood, is to find the parameter values that maximize the likelihood function. This might appear to be a strange criterion because, from our foregoing discussion of probability theory, it would seem more natural to maximize the probability of the parameters given the data, not the probability of the data given the parameteres. In fact these two criteria are related.\n",
    "\n",
    "## Bias of maximum likelihood\n",
    "\n",
    "$\\mathbb{E}[\\mu_{ML}] = \\mu$\n",
    "\n",
    "$\\mathbb{E}[\\sigma_{ML}^{2}] = \\left(\\dfrac{N-1}{N}\\right)\\sigma^{2}$\n",
    "\n",
    "We see that, when averaged over data sets of a given size, the maximum likelihood solution for the mean will equal to the true mean. However, the maximum likelihood estimate of the variance will underestimate the true variance by a factor of **(N-1)/N.**. This is an example of a phenomenon called bias in which the estimator of a random quantity is systematically different from the true value. \n",
    "\n",
    "You can easily fix the bias btw, by multiplying with N/N-1, correcting for the bias of maximum likelihood in complex models such as neural networks is not so easy, however. In the case of the gaussian, for anything other than small N, this bias will not prove to be a serious problem. However, we will be interested in complex models with many params, for which the bias problems associated with maximum likelihood will be much more severe. In fact, the issue of bias in maximum likelihood is closely related to the problem of over-fitting.\n",
    "\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "We have seen linear regression expressed in terms of error minimization. We view it from a probabilistic perspective. The goal in the regression problem is to be able to make predictions for the target variable $t$ given some new value of the input variable $x$ by using a set of training data comprising $N$ input values $\\bold{x} = (x_{1},...,x_{N})$ and their corresponding target values $\\bold{t} = (t_{1},...,t_{N})$. We can express our uncertainty over the value of the target variable using a probability distribution. For this purpose, we will assume that, given the value of x, the corresponding value of t has a Gaussian distribution with a mean equal to the value $y(x, w)$ of the poly curve, where w are the polynomial coefficients, and a variance $\\sigma^{2}$. Thus we have\n",
    "\n",
    "$p(t|x,w,\\sigma^{2}) = \\cal{N}(t|y(x,w),\\sigma^{2})$\n",
    "\n",
    "We can now use the training data ${\\bold{x}, \\bold{t}}$ to determine the values of the unknown parameters $w$ and $\\sigma^{2}$ by maximum likelihood.\n",
    "\n",
    "If the data is assumed to be drawn independently from the distribution. Then the likelihood function is given by.\n",
    "\n",
    "$p(\\bold{t}|\\bold{x},\\bold{w},\\sigma^{2}) = \\prod\\limits_{n=1}^{N}\\cal{N}(t_{n}|y(x_{n},w),\\sigma^{2})$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
