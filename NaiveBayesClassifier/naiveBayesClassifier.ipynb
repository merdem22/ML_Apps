{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24dd51be-047d-4ab8-94b8-7cdfd8b299dc",
   "metadata": {},
   "source": [
    "##### We are given a multivariate classficaition dataset, containing 11314 and 7532 documents. each document is represented as a 2000-dimensional binary vector\n",
    "##### Each feature shows whether a word appears in the corresponding document or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eae1b7-7d3a-45e7-9992-f40519bcdae0",
   "metadata": {},
   "source": [
    "## Import the libraries required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aa5841f-81c8-4b67-bb91-d403642f8b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e867587a-0e00-41ef-8e86-3ad0f7f90a22",
   "metadata": {},
   "source": [
    "### Read the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "553508f7-fb7c-48ac-a275-9e7b753416f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 2000)\n",
      "(7532, 2000)\n",
      "(11314,)\n",
      "(7532,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.genfromtxt(fname = \"20newsgroup_words_train.csv\", delimiter = \",\" ,dtype = int)\n",
    "X_test = np.genfromtxt(fname = \"20newsgroup_words_test.csv\", delimiter = \",\" ,dtype = int)\n",
    "Y_train = np.genfromtxt(fname = \"20newsgroup_labels_train.csv\", delimiter = \",\" ,dtype = int)\n",
    "Y_test = np.genfromtxt(fname = \"20newsgroup_labels_test.csv\", delimiter = \",\" ,dtype = int)\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e46ccb-ba18-4d8f-b386-2c112ee33d3f",
   "metadata": {},
   "source": [
    "### Estimating the prior probabilities\n",
    "\n",
    "$ \\widehat{\\Pr}(y = c) = \\dfrac{\\sum\\limits_{i = 1}^{N}\\mathbb{1}(y_{i} = c)}{N}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72196185-ab93-48b1-ac9d-f79b8eac1d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "N = Y_train.shape[0]\n",
    "K = np.max(Y_train)\n",
    "priors = [np.sum(Y_train == c)/N for c in range(1,K+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eacf86-741e-40a8-a8b0-cce1acf13858",
   "metadata": {},
   "source": [
    "### To have a naive bayes classifier after finding the priors we also need the likelihood probability with each attribute for each class, then we can calculate the posterior probability but remember to normalize)\n",
    "\n",
    "$ \\widehat{\\Pr}(y = c | x) = \\dfrac{\\hat{p}(x|y = c)\\widehat{Pr}(y=c)}{\\hat{p}(x)}$\n",
    "\n",
    "let\n",
    "\n",
    "$ \\pi_{cd} = p(x_{d} = 1 | y = c)$\n",
    "\n",
    "we will have 20 x 2000 of these\n",
    "\n",
    "$ \\pi_{cd} = \\dfrac{\\sum\\limits_{i=1}^{N}\\mathbb{1}(y_{i} = c, x_id = 1)}{N_{c}}$\n",
    "\n",
    "\n",
    "##### However we also have to add α to the numerator and αD to the denominator to avoid 0 probabilities (Laplace smoothing), we have set alpha to be 0.2\n",
    "\n",
    "$ \\pi_{cd} = \\dfrac{\\sum\\limits_{i=1}^{N}\\mathbb{1}(y_{i} = c, x_id = 1) + \\alpha}{N_{c} + \\alpha D}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29451643-8ee2-4519-9104-5bcbaee20574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 11314)\n",
      "[480 584 591 590 578 593 585 594 598 597 600 595 591 594 593 599 546 564\n",
      " 465 377]\n",
      "2000\n",
      "[ 5033  5334  2466 ... 36973  2382  4266]\n",
      "[[3.00022727e+00 1.10002273e+01 1.20002273e+01 ... 2.33000227e+02\n",
      "  1.20002273e+01 2.40002273e+01]\n",
      " [2.00002033e+01 1.10002033e+01 1.00002033e+01 ... 9.50002033e+01\n",
      "  6.00020325e+00 7.00020325e+00]\n",
      " [1.90002018e+01 1.00002018e+01 8.00020182e+00 ... 1.18000202e+02\n",
      "  2.01816347e-04 5.00020182e+00]\n",
      " ...\n",
      " [2.90002075e+01 8.40002075e+01 2.00020747e+00 ... 2.41000207e+02\n",
      "  2.80002075e+01 3.40002075e+01]\n",
      " [1.60002312e+01 3.00002312e+01 2.00023121e+00 ... 1.76000231e+02\n",
      "  1.00002312e+01 1.70002312e+01]\n",
      " [4.00025740e+00 7.00025740e+00 6.00025740e+00 ... 1.55000257e+02\n",
      "  1.20002574e+01 3.10002574e+01]]\n"
     ]
    }
   ],
   "source": [
    "a = 0.2\n",
    "print(X_train.T.shape)\n",
    "\n",
    "class_counts = np.array([np.sum(Y_train == c + 1) for c in range(K)])\n",
    "print(class_counts)\n",
    "\n",
    "D = X_train.shape[1]\n",
    "print(D)\n",
    "feature_counts = Y_train.T @ X_train\n",
    "\n",
    "pi = (feature_counts + a) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebae474-7798-47e2-9bb5-f4b642e0ebf0",
   "metadata": {},
   "source": [
    "### Now that we have the requirements, (prior and conditionals), we can calculate the score values.\n",
    "\n",
    "#### Remember how we calculate the score from the previous lab.\n",
    "\n",
    "You have to notice that p(x|y) is a multinomial distribution, and i guess we assume independence of the words in a data point here. \n",
    "\n",
    "$ g_{c}(x) = \\log\\left[\\prod\\limits_{d = 1}^{D}\\hat{p}(x_d | y = c)\\right] + \\log\\widehat{\\Pr}(y = c)$\n",
    "\n",
    "$ =  \\log\\left[\\prod\\limits_{d = 1}^{D}\\hat{\\pi}_{cd}^{x_{d}}(1-\\hat{\\pi}_{cd})^{1 - x_{d}}\\right] +\\log\\widehat{\\Pr}(y = c)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dc92496-9ac7-4248-8e9d-2f14835f63c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_9/k4h1mmcj1670zx23901mg27m0000gn/T/ipykernel_28665/419269719.py:4: RuntimeWarning: invalid value encountered in log\n",
      "  inv_log = np.log(1-pi)\n"
     ]
    }
   ],
   "source": [
    "## write the math equation then.\n",
    "\n",
    "log = np.log(pi)\n",
    "inv_log = np.log(1-pi)\n",
    "log_priors = np.log(priors)\n",
    "\n",
    "score_values = X_train @ log.T + X_train @ inv_log.T + log_priors\n",
    "\n",
    "print(score_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
